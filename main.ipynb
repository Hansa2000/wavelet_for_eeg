{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import iirnotch, filtfilt\n",
    "import random\n",
    "import pywt \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 173.61  # sampling freq of Adjek\n",
    "\n",
    "# Folder paths for each set\n",
    "folders = {\n",
    "    'A': './data/Z',  # Z000.txt - Z100.txt\n",
    "    'B': './data/O',  # O000.txt - O100.txt\n",
    "    'C': './data/N',  # N000.txt - N100.txt\n",
    "    'D': './data/F',  # F000.txt - F100.txt\n",
    "    'E': './data/S'   # S000.txt - S100.txt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the raw and filtered data\n",
    "raw_data_all_sets = {key: [] for key in folders.keys()}\n",
    "filtered_data_all_sets = {key: [] for key in folders.keys()}\n",
    "\n",
    "# Function to apply a notch filter to remove 50Hz noise\n",
    "def notch_filter(data, freq=50.0, fs=256.0, Q=30.0):\n",
    "    \"\"\"\n",
    "    Apply a notch filter to remove the 50Hz line noise.\n",
    "    Parameters:\n",
    "        data: 1D array of EEG data\n",
    "        freq: Frequency to remove (50 Hz)\n",
    "        fs: Sampling rate of the data\n",
    "        Q: Quality factor (controls bandwidth of the notch)\n",
    "    \"\"\"\n",
    "    w0 = freq / (fs / 2)  # Normalize frequency\n",
    "    b, a = iirnotch(w0, Q)  # Create notch filter\n",
    "    filtered_data = filtfilt(b, a, data)  # Apply filter\n",
    "    return filtered_data\n",
    "\n",
    "# Function to re-reference the data (average referencing)\n",
    "def re_reference(data):\n",
    "    \"\"\"\n",
    "    Re-reference the EEG data by subtracting the mean of the signal.\n",
    "    \"\"\"\n",
    "    #return data - np.mean(data)\n",
    "    return ((data - np.mean(data))/np.std(data))\n",
    "\n",
    "\n",
    "# Re-reference and filter data for each set\n",
    "for set_name, folder_path in folders.items():\n",
    "    # Find all .txt files in the folder\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    \n",
    "    # Iterate over each file and process the data\n",
    "    for file in txt_files:\n",
    "        # Load the data (adjust the delimiter if needed)\n",
    "        data = pd.read_csv(file, delimiter=\"\\t\", header=None)\n",
    "        \n",
    "        # Assuming each file contains a single column of EEG data\n",
    "        eeg_data = data[0].values\n",
    "        \n",
    "        # Store raw EEG data before filtering\n",
    "        raw_data_all_sets[set_name].append(eeg_data)\n",
    "        \n",
    "        # Re-reference the EEG data (average reference)\n",
    "        re_referenced_data = re_reference(eeg_data)\n",
    "        \n",
    "        # Apply a notch filter to remove 50Hz noise\n",
    "        filtered_eeg_data = notch_filter(re_referenced_data, freq=50.0, fs=256.0, Q=30.0)\n",
    "        \n",
    "        # Append the filtered data to the respective set\n",
    "        filtered_data_all_sets[set_name].append(filtered_eeg_data)\n",
    "\n",
    "# Now raw_data_all_sets contains raw EEG data and filtered_data_all_sets contains re-referenced and filtered EEG data for all sets\n",
    "for set_name in raw_data_all_sets.keys():\n",
    "    print(f\"Raw data loaded for Set {set_name}: {len(raw_data_all_sets[set_name])} files.\")\n",
    "    print(f\"Filtered data loaded for Set {set_name}: {len(filtered_data_all_sets[set_name])} files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 23.6  # Assuming the duration of the EEG signal is 23.6 seconds\n",
    "\n",
    "# To store the selected raw and filtered data for plotting\n",
    "selected_raw_data = {}\n",
    "selected_filtered_data = {}\n",
    "\n",
    "# Iterate through each set and randomly select the same file's raw and filtered data\n",
    "for set_name in raw_data_all_sets.keys():\n",
    "    \n",
    "    # Randomly select the index of a file from the list of files\n",
    "    random_index = random.randint(0, len(raw_data_all_sets[set_name]) - 1)\n",
    "    \n",
    "    # Use the same index to select the raw and corresponding filtered data\n",
    "    random_raw_eeg_data = raw_data_all_sets[set_name][random_index]\n",
    "    random_filtered_eeg_data = filtered_data_all_sets[set_name][random_index]\n",
    "    \n",
    "    print(f\"Randomly selected raw and filtered data for Set {set_name} (File Index: {random_index})\")\n",
    "    \n",
    "    # Store the selected data for plotting\n",
    "    selected_raw_data[set_name] = random_raw_eeg_data\n",
    "    selected_filtered_data[set_name] = random_filtered_eeg_data\n",
    "\n",
    "# Plotting the randomly selected raw and filtered data from each set\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Generate time points assuming the same sampling rate and duration\n",
    "time_points = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "\n",
    "# Plot each set's raw and filtered data on the same subplot\n",
    "for i, set_name in enumerate(selected_raw_data.keys(), 1):\n",
    "    plt.subplot(3, 2, i)  # Create a 3x2 subplot grid\n",
    "    \n",
    "    # Plot raw EEG data\n",
    "    plt.plot(time_points, selected_raw_data[set_name], label='Raw Data', alpha=0.7)\n",
    "    \n",
    "    # Plot filtered EEG data\n",
    "    plt.plot(time_points, selected_filtered_data[set_name], label='Filtered Data', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Set {set_name} - Raw vs Filtered EEG Data')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('EEG Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sets_data = {key: np.array(val) for key, val in filtered_data_all_sets.items()}\n",
    "\n",
    "# Function to compute energy of wavelet coefficients\n",
    "def compute_energy(coefficients):\n",
    "    return np.sum(np.square(coefficients))\n",
    "\n",
    "# Wavelet to use for DWT\n",
    "wavelet = 'coif1'\n",
    "\n",
    "# level of decomposition\n",
    "dlevel = 3\n",
    "\n",
    "# To store feature vectors for each set\n",
    "feature_vectors_B = []\n",
    "feature_vectors_E = []\n",
    "\n",
    "# Process Set B (Blue)\n",
    "for eeg_data in all_sets_data['B']:\n",
    "    coeffs_B = pywt.wavedec(eeg_data, wavelet, level=dlevel)  # 3 levels of decomposition\n",
    "    d2_B = coeffs_B[2]  # D2 coefficients (second detail level)\n",
    "    d3_B = coeffs_B[3]  # D3 coefficients (third detail level)\n",
    "    \n",
    "    # Compute energy for D2 and D3\n",
    "    d2_energy_B = compute_energy(d2_B)\n",
    "    d3_energy_B = compute_energy(d3_B)\n",
    "    \n",
    "    # Create feature vector for Set B\n",
    "    feature_vectors_B.append([d2_energy_B, d3_energy_B])\n",
    "\n",
    "# Process Set E (Red)\n",
    "for eeg_data in all_sets_data['E']:\n",
    "    coeffs_E = pywt.wavedec(eeg_data, wavelet, level=dlevel)  # 3 levels of decomposition\n",
    "    d2_E = coeffs_E[2]  # D2 coefficients (second detail level)\n",
    "    d3_E = coeffs_E[3]  # D3 coefficients (third detail level)\n",
    "    \n",
    "    # Compute energy for D2 and D3\n",
    "    d2_energy_E = compute_energy(d2_E)\n",
    "    d3_energy_E = compute_energy(d3_E)\n",
    "    \n",
    "    # Create feature vector for Set E\n",
    "    feature_vectors_E.append([d2_energy_E, d3_energy_E])\n",
    "\n",
    "# Convert feature vectors to numpy arrays for easy plotting\n",
    "feature_vectors_B = np.array(feature_vectors_B)\n",
    "feature_vectors_E = np.array(feature_vectors_E)\n",
    "\n",
    "# Plot the feature vectors\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting feature vectors for Set B\n",
    "plt.scatter(feature_vectors_B[:, 0], feature_vectors_B[:, 1],  marker='+', color='blue', label='Set B (D2 Energy vs D3 Energy)', alpha=0.6)\n",
    "\n",
    "# Plotting feature vectors for Set E\n",
    "plt.scatter(feature_vectors_E[:, 0], feature_vectors_E[:, 1], marker='+', color='red', label='Set E (D2 Energy vs D3 Energy)', alpha=0.6)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.title('Feature Vectors: Energy D2 vs Energy D3')\n",
    "plt.xlabel('Energy D2')\n",
    "plt.ylabel('Energy D3')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sets_data = {key: np.array(val) for key, val in filtered_data_all_sets.items() if key in ['C', 'D', 'E']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "def calculate_energy(coeffs):\n",
    "    \"\"\"\n",
    "    Calculate the energy for detail coefficients (ED_i) and approximation coefficients (EA_i)\n",
    "    using the formulas provided in the paper.\n",
    "    \"\"\"\n",
    "    ED = np.sum(np.square(coeffs[1]))  # Energy of detail coefficients\n",
    "    EA = np.sum(np.square(coeffs[0]))  # Energy of approximation coefficients\n",
    "    return ED, EA\n",
    "\n",
    "def wavelet_decomposition(signal, wavelet, level):\n",
    "    \"\"\"\n",
    "    Perform wavelet decomposition up to the specified level using the provided wavelet function.\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "    return coeffs\n",
    "\n",
    "# Assuming data is already preprocessed and normalized\n",
    "def prepare_data(all_signals):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing (7:3 ratio), then perform 10-fold cross-validation.\n",
    "    \"\"\"\n",
    "    # Convert list of signals into features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for set_name, signals in all_signals.items():\n",
    "        label = 1 if set_name == 'E' else 0  # Assuming 'E' is epileptic, others are normal\n",
    "        for signal in signals:\n",
    "            # Extract wavelet features for different levels\n",
    "            coeffs = wavelet_decomposition(signal, 'coif1', level=6)  # Set level=6 initially\n",
    "            ED, EA = calculate_energy(coeffs)\n",
    "            features.append([ED, EA])\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# SVM Training Function\n",
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    svm = SVC(kernel='rbf')  # Using RBF kernel, you can try others like 'linear', 'poly', etc.\n",
    "    svm.fit(X_train, y_train)\n",
    "    accuracy = svm.score(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_decomposition_levels_with_svm(all_signals, max_level=10):\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over different decomposition levels (1 to max_level)\n",
    "    for level in range(1, max_level+1):\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for set_name, signals in all_signals.items():\n",
    "            label = 1 if set_name == 'E' else 0  # Epileptic signal label\n",
    "            for signal in signals:\n",
    "                coeffs = wavelet_decomposition(signal, 'coif1', level=level)\n",
    "                ED, EA = calculate_energy(coeffs)\n",
    "                features.append([ED, EA])\n",
    "                labels.append(label)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Train SVM and get accuracy\n",
    "        accuracy = train_svm(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        results.append((level, accuracy))\n",
    "        print(f\"Decomposition Level {level} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_bar_results_with_line(results):\n",
    "    levels = [r[0] for r in results]\n",
    "    accuracies = [r[1] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a bar plot with thinner bars\n",
    "    plt.bar(levels, accuracies, color='black', width=0.4)\n",
    "    \n",
    "    # Draw a line through the tops of the bars\n",
    "    plt.plot(levels, accuracies, marker='o', linestyle=':', color='black', linewidth=1.5)\n",
    "    \n",
    "    # Set x-axis and y-axis ticks\n",
    "    plt.xticks(np.arange(1, 11, 1))  # X-axis ticks from 1 to 10\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))  # Y-axis ticks from 0 to 1 in increments of 0.1\n",
    "    \n",
    "    # Adding labels and title\n",
    "    plt.title('Decomposition Level vs Classification Accuracy (SVM)')\n",
    "    plt.xlabel('Wavelet Decomposition Level')\n",
    "    plt.ylabel('Classification Accuracy')\n",
    "    \n",
    "    # Display grid\n",
    "    # plt.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "results = evaluate_decomposition_levels_with_svm(all_sets_data, max_level=10)\n",
    "plot_bar_results_with_line(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "def calculate_energy(coeffs):\n",
    "    \"\"\"\n",
    "    Calculate the energy for detail coefficients (ED_i) and approximation coefficients (EA_i)\n",
    "    using the formulas provided in the paper.\n",
    "    \"\"\"\n",
    "    ED = np.sum(np.square(coeffs[1]))  # Energy of detail coefficients\n",
    "    EA = np.sum(np.square(coeffs[0]))  # Energy of approximation coefficients\n",
    "    return ED, EA\n",
    "\n",
    "def wavelet_decomposition(signal, wavelet, level):\n",
    "    \"\"\"\n",
    "    Perform wavelet decomposition up to the specified level using the provided wavelet function.\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "    return coeffs\n",
    "\n",
    "# Assuming data is already preprocessed and normalized\n",
    "def prepare_data(all_signals):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing (7:3 ratio), then perform 10-fold cross-validation.\n",
    "    \"\"\"\n",
    "    # Convert list of signals into features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for set_name, signals in all_signals.items():\n",
    "        label = 1 if set_name == 'E' else 0  # Assuming 'E' is epileptic, others are normal\n",
    "        for signal in signals:\n",
    "            # Extract wavelet features for different levels\n",
    "            coeffs = wavelet_decomposition(signal, 'coif1', level=6)  # Set level=6 initially\n",
    "            ED, EA = calculate_energy(coeffs)\n",
    "            features.append([ED, EA])\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# SVM Training Function\n",
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    svm = SVC(kernel='rbf')  # Using RBF kernel, you can try others like 'linear', 'poly', etc.\n",
    "    svm.fit(X_train, y_train)\n",
    "    accuracy = svm.score(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_decomposition_levels_with_svm(all_signals, max_level=10):\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over different decomposition levels (1 to max_level)\n",
    "    for level in range(1, max_level+1):\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for set_name, signals in all_signals.items():\n",
    "            label = 1 if set_name == 'E' else 0  # Epileptic signal label\n",
    "            for signal in signals:\n",
    "                coeffs = wavelet_decomposition(signal, 'db2', level=level)\n",
    "                ED, EA = calculate_energy(coeffs)\n",
    "                features.append([ED, EA])\n",
    "                labels.append(label)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Train SVM and get accuracy\n",
    "        accuracy = train_svm(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        results.append((level, accuracy))\n",
    "        print(f\"Decomposition Level {level} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_bar_results_with_line(results):\n",
    "    levels = [r[0] for r in results]\n",
    "    accuracies = [r[1] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a bar plot with thinner bars\n",
    "    plt.bar(levels, accuracies, color='black', width=0.4)\n",
    "    \n",
    "    # Draw a line through the tops of the bars\n",
    "    plt.plot(levels, accuracies, marker='o', linestyle=':', color='black', linewidth=1.5)\n",
    "    \n",
    "    # Set x-axis and y-axis ticks\n",
    "    plt.xticks(np.arange(1, 11, 1))  # X-axis ticks from 1 to 10\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))  # Y-axis ticks from 0 to 1 in increments of 0.1\n",
    "    \n",
    "    # Adding labels and title\n",
    "    plt.title('Decomposition Level vs Classification Accuracy (SVM)')\n",
    "    plt.xlabel('Wavelet Decomposition Level')\n",
    "    plt.ylabel('Classification Accuracy')\n",
    "    \n",
    "    # Display grid\n",
    "    # plt.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "results = evaluate_decomposition_levels_with_svm(all_sets_data, max_level=10)\n",
    "plot_bar_results_with_line(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy and time\n",
    "def evaluate_svm(X_train, X_test, y_train, y_test):\n",
    "    # Initialize the SVM model\n",
    "    svm = SVC(kernel='rbf')  # You can change the kernel as needed\n",
    "    \n",
    "    # Fit the model\n",
    "    start_train = time.time()\n",
    "    svm.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    # Measure prediction time\n",
    "    start_predict = time.time()\n",
    "    y_pred = svm.predict(X_test)\n",
    "    end_predict = time.time()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100  # Convert to percentage\n",
    "    \n",
    "    # Calculate classification time (seconds per sample)\n",
    "    classification_time = (end_predict - start_predict) / len(X_test)\n",
    "    \n",
    "    # Return accuracy and classification time\n",
    "    return accuracy, classification_time\n",
    "\n",
    "def evaluate_wavelet_data(all_signals, wavelet, level):\n",
    "    \"\"\"\n",
    "    Perform wavelet decomposition for different features (Energy, STD, etc.) and evaluate SVM.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for feature in ['Energy', 'STD', 'Entropy', 'Energy + STD']:\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for set_name, signals in all_signals.items():\n",
    "            label = 1 if set_name == 'E' else 0  # Epileptic signal label\n",
    "            for signal in signals:\n",
    "                # Perform wavelet decomposition\n",
    "                coeffs = wavelet_decomposition(signal, wavelet, level)\n",
    "                \n",
    "                # Calculate energy or other features\n",
    "                if feature == 'Energy':\n",
    "                    ED, _ = calculate_energy(coeffs)\n",
    "                    feature_vector = [ED]\n",
    "                elif feature == 'STD':\n",
    "                    _, EA = calculate_energy(coeffs)\n",
    "                    feature_vector = [np.std(coeffs[0])]\n",
    "                elif feature == 'Entropy':\n",
    "                    entropy = -np.sum(np.log2(np.abs(coeffs[0])) * np.abs(coeffs[0]))\n",
    "                    feature_vector = [entropy]\n",
    "                elif feature == 'Energy + STD':\n",
    "                    ED, EA = calculate_energy(coeffs)\n",
    "                    feature_vector = [ED, np.std(coeffs[0])]\n",
    "                \n",
    "                features.append(feature_vector)\n",
    "                labels.append(label)\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Evaluate SVM for the feature and wavelet\n",
    "        accuracy, classification_time = evaluate_svm(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Store results\n",
    "        results[feature] = {'CA(%)': accuracy, 'CT(S)': classification_time}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "wavelets = ['db2', 'haar', 'coif1', 'bior1.1']\n",
    "max_level = 6\n",
    "\n",
    "# Dictionary to store results\n",
    "final_results = {}\n",
    "\n",
    "for wavelet in wavelets:\n",
    "    for level in range(1, max_level + 1):\n",
    "        print(f\"Evaluating wavelet: {wavelet}, Level: {level}\")\n",
    "        results = evaluate_wavelet_data(all_sets_data, wavelet, level)\n",
    "        final_results[(wavelet, level)] = results\n",
    "\n",
    "# Convert final results to a pandas DataFrame for easy display\n",
    "df_results = pd.DataFrame.from_dict({(i,j): final_results[i,j] for i,j in final_results.keys()}, orient='index')\n",
    "\n",
    "# Display the table with features, wavelets, and decomposition levels\n",
    "print(df_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table using tabulate for console-friendly output\n",
    "print(tabulate(df_results, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Export to LaTeX (if needed)\n",
    "latex_table = df_results.to_latex()\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
